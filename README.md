<!-- ![](./static/pic/Chinchilla_pic.png) -->
<p align="center">
  <img width="650" height="350" src="./static/pic/4.png">
</p>

# ä¸­æ–‡é€šç”¨ä¿¡æ¯æŠ½å–å¤§æ¨¡å‹(**Chinchilla**)
[![LICENSE](https://img.shields.io/hexpm/l/plug?label=license)](https://github.com/hccngu/Chinchilla/blob/main/LICENSE)
[![torch](https://img.shields.io/badge/pytorch-%3E=1.13-red?logo=pytorch)](https://pytorch.org/)
<!-- [![data](https://img.shields.io/badge/huggingface-dataset-yellow?logo=data:image/png;base64,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)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT) -->
<!-- [![model](https://img.shields.io/badge/huggingface-model-yellow?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAABmJLR0QA/wD/AP+gvaeTAAAJXUlEQVRYCQXBeWzW933A8TfYQIBAMOa0/Xx+QJZoaRJIQpusySJSaGKY0nWpRrZVmTq1Ug+t/zRd2/2zqUv/WNdKW9RN3SZtU7NpUmqwuQyGmPswl/Hx/T0c7ZIWSihgG/P4enxgP++9XgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPDRHVf+us9nb/b7ym8G3XRz0PXXbrkMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICP+3x8YNi/nZnxqOWPb3uvc8r+49p/Su+nSSfvfTwx6ZH+ocp37pUrBQAAAAAAAAAAAAAAAAAAAAAAAPrHrBsq+6+OfjTojZ/omVe1dZW2zNPm2dpSpbsWaNta7fxTvdtsZWroTmnEH6k1AAAAAAAAAAAAAAAAAAAA9A/5OadHP/SjH+q+ldqCts/XjmXaWadd9XqpXi+s1lNLtW2O7kTbN2j/HqemvVwu+xIAAAAAAAAAAAAAAAAA/aXK1xz/sOyJl3UnerJGewpazDTPNA/NQ/PQPNM80zzTrnr9YKE2zdL0TXXy/vCYbwAAAAAAAAAAAAAAcKfknzl2ddK2dbqvWrsatJhpCk2hKTSFptAUmkJTaArNQ4uZnlupO9FzX3DGyZGRcT8DAAAAAAAAAADAbwf8RGWqr8/2p7R1rvaG5qEpNIWm0BSaQlNoCk2hKTSFptAUWsy0s053oj1fdXLa34yOVlYDAAAAAAAAoM4en3S/6Su6C+0paB6aQlNoMTQPTaEpNIWm0BSaQouheWgKTaHFTM+t0ia0f4elMf8bAAAAAAAAgJv33ObA0Rl3ztZzKzXPNIWm0GJ47/Q6p3syLYam0BSaQlNoseDgmbVOdWdaLGgKTaHFTI8s0gPr9EFpfHTUZwAAAAAAABifcqeX3tT9czTPNIWmsHK54H/9/bNu/swW3/7qi45cWKN5aApNoVcK/vzdDW7ZvMVvfOn3vdexVouhKTSFdhd0J3r7Zw6O+O8AAAAAAFy5XlldGbrW594lenaF5pmm0Dwcv7jG17dt9plPNrrhuUYv7fhdvVLQFJqH0z3hm29scsPGRp96ZqvH3ntSrxY0habQYqYfzNdTW5zRX97SBQAAAABc7/N1b72vzWhPQVNoCk1hpRj+w/c+5Q+/97zH/ucTDp5Zq3loCk2hKTz7/hPu+enTfvmLL/vb449qMTSFptA807PLdU+Njt140He/8hwAAAAAt+/7Xf/v73TvLM0zTaEpNIXm4UxvWEmh1xo0D02hKTSFptArBb1ScKor02JoCk2hKTQPvVSvzbN1qMP+kl8EAAAAYHDEH1v8uh6o1jzTFJpCU2gKTaF56NWCXi5oCk2hKTQPvVrQywXNQ1NoCk2hKTSF9hS0BR3Y6537fhsAAACAUtl/Mn1FD87RPNMUmkJTaAq9WvDoe0/6ztsv+IsDj2mxoL9o0GsNjnWucd+/Pe33v/WC5YtrNA9NoSk0habQnoK2oH3N3h32rwEAAAC4N+IPvPot3T9bi2s0zzTPNIXmodcKHvrPp3xyw1Y3bfqsf/nll/zHv/mk7/zV827/o00+s7HR7Z9/xanuTPNM89AUmmeaZ9pT0GZ08JD9w5VvAAAAAHD9buXPvf4T3YUeq9G2hdqxQvNMO+u0u86B0+v8g8bNPvvCa776e42++HyjW15o9OVPv+b6jY3+y/c36rWCdtbppXrNQ0/VattCPbxYW+bpcKrcvu8rAAAA2NRUdbfkuw60a9MsbanWtoXaXK0HH9aWam2u1gu17vjper/00lY/3Pp52xtft3fb52zf/Id+bfsmBzrW6cka3Vmlu+Zo20Jtrta2hboD3R86MTAzNOYPbGqqAgBgqOy3HTyg+x/Vptm6d57mmR6r0fdn6bEaPfKI7p6jeYOl/13v2De3OPrWNkf+Yqvj77zkxOlHtbdOm6v1xFJtX6Q/n6VnlmlvQVuqdcc8PbRey10OlX0bgCatcmaq0+Mb9eA8PVGrLXP06BLNM22dr2eWa0+DNlfryVq9FprCyoW1Vi6t0SsFvZLp4Ud091zNMz1Wo4ce1mKmhxbp7rl6apnum6UX3nB8qpKamqxCrVJ7PPmyts7VnoJ21ev++XpiqV5cra3ztbtBjy3R5mq9VKd5pnloHppnen6l7qzS08u0s05bH9Kuej28WA8u1N6CdjXortl66S3Lk15Vq7hxzyctpRH31WrrfG1doKeXa29BP1ikHcv1WI2eqtUU2jpf987T7gZNoXmmnXXaMkcPPax5pkeX6KlaPVmrRx7R3oIer9V983XfPD34O1Ym+oYHhn2CW/dsdPC0Ns/W86u0u0GPLNFDi/RSnXbVa55pCs1Duxt091xtX6x5pin0wAJtfUh7C5pCU2ieaVe9XlytbQ/r8aXaW9DTy3TXIh3/taURP8vNQTdYujzp7oXatlCPL9ULq/X8Kr24WvNMU2gKTaHFTE/W6u452lvQrnptrtazKzTPNIWm0BSaZ3p+lV5YredX6bGl2vqQ7i/oRP/kyKRPc3vAFxzs1J2ztGO5djXokRo9u1KLmabQFJpCU2gx0xNLdc9czTPtadCWaj27QvNMU2gKTaEptJjpqeV6bKl2F/TEEt29RMu3LI1VPsXAgItnZrzi1e9oE3pwnl5Ypb2hxUyLmeaZ5pkWM+1u0N1V2oIeWaCH5+sOtHWeptBipnmmeabFTPNMewp6bqUeqNYd1Xr9XScf+Et1EQDlqcqL0zP+ynsH9NSr2vyQ7kIPztUTj+iZZdqxTI8v1ha07THtfFM7XtOz27TrLd1Xp3vQE0u0Y7meqdXji7Vtjraguxbp2S/ocIdT016fmvLTAAAAjI5WVpfG/LH6K0eLeuOf9cJ2bV+vrfXaWqcfrNcr39XyTccnPTE97eGp6Ur72KTHHb6svV/XQ0/ovtXaWtDDz2nnW/rxf+j4Rz6Y9sbwuD9SVwEAAAAAAFAqWVMqu/3+iO+NT9lTmZ6443j/mON95cr0xJ3RCU+WJ90OAAAAUCr7J2MTnqg8KN9x/G7ZiYGxyvTknfKEvaUxfzY85h+XStYAAAAAAAAAAAAAAKDOHh52ed+Qjw1N+PjtEVcAAAAAAAAAANweccXQhI/3DfnY8LDL1dkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD8P8HSw4EMlPZhAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIzLTA0LTEyVDEyOjI0OjQxKzAwOjAwUmNguAAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMy0wNC0xMlQxMjoyNDo0MSswMDowMCM+2AQAAAAodEVYdGRhdGU6dGltZXN0YW1wADIwMjMtMDQtMTJUMTI6MjQ6NDErMDA6MDB0K/nbAAAAAElFTkSuQmCC)](https://huggingface.co/QingyiSi/Alpaca-CoT) -->
<!-- [![colab](https://img.shields.io/badge/Google-Colab-blue?logo=Google%20Colab)](https://colab.research.google.com/drive/1wfrKqyPkz5BGD1Gkij_cvbUeweIDdRav?usp=sharing) -->

è¿™æ˜¯Chinachillaé¡¹ç›®çš„å­˜å‚¨åº“ï¼Œè¯¥é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªå¤§å‹ä¸­æ–‡é€šç”¨ä¿¡æ¯æŠ½å–æ¨¡å‹ã€‚
<!-- æˆ‘ä»¬é¦–å…ˆæ”¶é›†å¹¶æ„å»ºå…·æœ‰å¹¿æ³›çš„æŒ‡ä»¤é›†åˆå’Œç”¨äºå„ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠå„ç§å‚æ•°æ•ˆç‡æ–¹æ³•(å¦‚LoRAï¼ŒP-Tuning)çš„ç»Ÿä¸€æ¥å£ã€‚æˆ‘ä»¬æ­£åœ¨ä¸æ–­æ‰©å±•æˆ‘ä»¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®æ”¶é›†ï¼Œå¹¶é›†æˆæ›´å¤šçš„LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ–°å»ºäº†ä¸€ä¸ªåˆ†æ”¯[`tabular_llm`](https://github.com/PhoebusSi/Alpaca-CoT/tree/tabular_llm)æ¥æ„é€ å¯ä»¥å¤„ç†è¡¨æ ¼æ™ºèƒ½ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ -->

<!-- <img src="./figures/image.png" width = "100" height = "100" align=right /> -->
æ¬¢è¿æ‚¨å‘æˆ‘ä»¬æä¾›ä»»ä½•æœªæ”¶é›†çš„ä¿¡æ¯æŠ½å–æ•°æ®é›†(æˆ–å…¶æ¥æº)ã€‚æˆ‘ä»¬å°†ç»Ÿä¸€å®ƒä»¬çš„æ ¼å¼ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬æ‰€æ„å»ºçš„instructionsèå…¥ç»Ÿä¸€çš„æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡è¯¥ç»Ÿä¸€æ•°æ®é›†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿›è¡Œå¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œå¹¶å¼€æºæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„é¡¹ç›®èƒ½å¤Ÿä¸ºä¿¡æ¯æŠ½å–æ¨¡å‹çš„å¼€æºè¿›ç¨‹åšå‡ºå¾®è–„çš„è´¡çŒ®ï¼Œé™ä½ä¿¡æ¯æŠ½å–ä»»åŠ¡çš„éš¾åº¦ã€‚

<!-- æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©åŠ å…¥æˆ‘ä»¬çš„ç¾¤èŠ(WeChat)ï¼Œå’Œæ›´å¤šçš„åŒå¥½ç ”ç©¶è€…ä»¬äº¤æµã€‚ç›®å‰ç¾¤èŠäººæ•°è¿‡å¤šï¼Œéœ€è¦å¥½å‹é‚€è¯·æ‰èƒ½å…¥ç¾¤ï¼Œè¯·æ‰«ç åŠ æˆ‘ä¸ºå¥½å‹ï¼Œæ‹‰æ‚¨å…¥ç¾¤ã€‚ -->

<!-- ## News -->
<!-- - ğŸš€5.5: æ–°å»ºäº†ä¸€ä¸ªåˆ†æ”¯[`tabular_llm`](https://github.com/PhoebusSi/Alpaca-CoT/tree/tabular_llm)æ¥æ„é€ å¯ä»¥å¤„ç†å¤šç§è¡¨æ ¼æ™ºèƒ½ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- ğŸš€5.4: PEFTä¸­æ‰€æœ‰parameter-efficientæ–¹æ³•ï¼ˆå¦‚P-tuningï¼‰å‡è¢«é›†æˆè¿›æ¥ï¼Œå¯é€šè¿‡è¶…å‚ç®€å•è®¾ç½®ã€‚
- ğŸš€5.4: LLM `MOSS`å·²è¢«é›†æˆè¿›æ¥ã€‚
- 4.21: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›† `GAOKAO`, `camel`, `FLAN-Muffin`, `COIG`. 
- 4.15: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›† `webGPT`, `dolly`, `baize`, `hh-rlhf`, `OIG(part)`. 
- 4.12: ç°åœ¨ä½ å¯ä»¥åœ¨<a href="https://colab.research.google.com/drive/1wfrKqyPkz5BGD1Gkij_cvbUeweIDdRav?usp=sharing" >Google Colab</a>ä¸­ä½“éªŒAlpaca-CoT.
- 4.11: æ·»åŠ äº†`å¤šè½®å¯¹è¯`åŠŸèƒ½ï¼Œæ„Ÿè°¢[@paulcx](https://github.com/paulcx)ã€‚
- 4.9: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›† `firefly`, `instruct`, `Code Alpaca` [è¿™é‡Œ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main).
- 4.7: æ·»åŠ äº†`å‚æ•°åˆå¹¶`ã€`æœ¬åœ°ä½¿ç”¨`ã€`æ‰¹é‡é¢„æµ‹`ã€`webæœåŠ¡`åŠŸèƒ½ï¼Œæ„Ÿè°¢[@weberrr](https://github.com/weberrr)ã€‚
- 4.4: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›†`FastChat`, `GPTeacher`,`Guanaco`,`HC3`,`prosocial-dialog`, `belle-chat&belle-math`, `xP3` å’Œ `natural-instructions`ã€‚
- 4.3: ä¸­æ–‡çš„CoTæ•°æ®é›†`CoT_CN_data.json`å·²ä¸Šä¼ åˆ°[è¿™é‡Œ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main).
- 4.1: åœ¨instinwild-CN(47k) + belle(1.5M)å¾®è°ƒå¾—åˆ°çš„Bloom7bçš„`checkpoint`å·²è¢«ä¸Šä¼ åˆ°äº†[è¿™é‡Œ](https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main).
- 4.1: `instnwild`ï¼ˆæ”¶é›†è‡ªæ¨ç‰¹ï¼Œä¸»è¦æ˜¯ç”Ÿæˆã€å¼€æ”¾å¼QAå’Œmind-storm typesï¼‰å·²ç»è¢«ç»Ÿä¸€æ ¼å¼åŒ–å¹¶æ”¶é›†. -->


<!-- ## 0. ChatGPTèƒŒåçš„æŠ€æœ¯

**LLM**: ï¼ˆ **Large Language Models**ï¼‰æŒ‡ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒä¸”ä½“é‡è¾ƒå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œä¸€èˆ¬æ˜¯transformer-basedæ¨¡å‹ã€‚

**IFT**: ï¼ˆ **Instruction Fine-Tuning**ï¼‰æŒ‡ä»¤å¾®è°ƒï¼ŒæŒ‡ä»¤æ˜¯æŒ‡ç”¨æˆ·ä¼ å…¥çš„ç›®çš„æ˜ç¡®çš„è¾“å…¥æ–‡æœ¬ï¼ŒæŒ‡ä»¤å¾®è°ƒç”¨ä»¥è®©æ¨¡å‹å­¦ä¼šéµå¾ªç”¨æˆ·çš„æŒ‡ä»¤ã€‚

**CoT**: ï¼ˆ **Chain-of-Thought**ï¼‰æŒ‡ä»¤å½¢å¼çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼ŒåŒ…å«step-by-stepçš„æ¨ç†è¿‡ç¨‹ã€‚å¦‚ä¸‹å›¾è“è‰²éƒ¨åˆ†æ‰€ç¤ºã€‚

![cot](./figures/cot.jpg) -->

<!-- ## 1. å®šä½ -->

<!-- ChatGPTçš„å‡ºç°éªŒè¯äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨é€šç”¨äººå·¥æ™ºèƒ½(AGI)ä¸Šçš„æ½œåŠ›ã€‚åŸºäºLLaMA[1]ç­‰Large Language Models(LLMs)çš„instruction-tuningç ”ç©¶(å¦‚ï¼ŒAlpaca[2])å¤§å¹…åº¦åŠ é€Ÿäº†å¤ç°ChatGPTçš„è¿›ç¨‹ã€‚**Alpaca-CoT**å¸Œæœ›åœ¨è¿™ä¸ªç ”ç©¶æ–¹å‘ä¸Šåšå‡ºé€‚åº¦çš„è´¡çŒ®ï¼Œä»¥æ¨è¿›LLMsçš„å¼€æºè¿›ç¨‹ã€é™ä½LLMsç ”ç©¶å’Œä½¿ç”¨æˆæœ¬ã€‚

å…·ä½“æ¥è¯´ï¼Œ**Alpaca-CoT**é¡¹ç›®æ—¨åœ¨æ¢ç©¶å¦‚ä½•æ›´å¥½åœ°é€šè¿‡instruction-tuningçš„æ–¹å¼æ¥è¯±å¯¼LLMå…·å¤‡ç±»ä¼¼ChatGPTçš„äº¤äº’å’Œinstruction-followingèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¹¿æ³›æ”¶é›†äº†ä¸åŒç±»å‹çš„instructionï¼ˆå°¤å…¶æ˜¯Chain-of-Thoughtæ•°æ®é›†ï¼‰ï¼Œå¹¶åŸºäºLLaMAç»™å‡ºäº†æ·±å…¥ç»†è‡´çš„å®è¯ç ”ç©¶ï¼Œä»¥ä¾›æœªæ¥å·¥ä½œå‚è€ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–ä¸ªå°†CoTæ‹“å±•è¿›Alpacaçš„å·¥ä½œï¼Œå› æ­¤ç®€ç§°ä¸º"**Alpaca-CoT**"ã€‚


çƒ­çƒˆæ¬¢è¿æ‚¨å‘æˆ‘ä»¬æä¾›ä»»ä½•æœªè¢«æœ¬é¡¹ç›®æ”¶é›†çš„instruction-tuningåŠå„ç±»tasksæ•°æ®é›†ï¼ˆæˆ–å…¶æ¥æºï¼‰ã€‚æˆ‘ä»¬å°†ï¼š
- å°†è¿™äº›æ•°æ®æ”¶å½•å¹¶è¿›è¡Œç»Ÿä¸€æ ¼å¼åŒ–å¤„ç†ï¼›
- ç”¨è¿™äº›æ•°æ®é›†instruct fine-tune LLaMAæ¨¡å‹ï¼ˆæœªæ¥å°†é›†æˆæ›´å¤šLLMsï¼‰ï¼Œå¹¶å¼€æºå…¶checkpointï¼›
- è¿›è¡Œå¹¿æ³›çš„å®è¯ç ”ç©¶ä»¥æ¢ç©¶æ–°æ”¶å½•çš„æ•°æ®é›†çš„ä½œç”¨ã€‚


æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„é¡¹ç›®èƒ½å¤Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æºè¿‡ç¨‹åšå‡ºé€‚åº¦çš„è´¡çŒ®ï¼Œå¹¶é™ä½NLPç ”ç©¶äººå‘˜ä¸Šæ‰‹LLMç›¸å…³ç ”ç©¶çš„é—¨æ§›ã€‚ -->

<!-- ## 2. æ¦‚è¿° -->

<!-- è¿‘æœŸï¼Œ[LLaMA](https://arxiv.org/abs/2302.13971v1)[1]æ˜¾ç¤ºå‡ºæƒŠäººçš„zero-shotå’Œfew-shotèƒ½åŠ›ï¼Œä»…éœ€è¾ƒå°‘çš„å‚æ•°å³å¯å’ŒGPT-3.5æ€§èƒ½ç›¸å½“ï¼ˆLLaMA-13Bæ˜¾è‘—ä¼˜äºGPT-3ï¼ˆ175Bï¼‰ï¼ŒLLaMA-65Bä¸PaLM-540MBç›¸å½“ï¼‰ï¼Œæ˜æ˜¾é™ä½äº†è®­ç»ƒã€å¾®è°ƒå’Œä½¿ç”¨competitiveå¤§å‹è¯­è¨€æ¨¡å‹çš„æˆæœ¬ã€‚æœ€è¿‘ï¼Œä¸ºäº†æé«˜LLaMAçš„instruction-followingèƒ½åŠ›ï¼Œ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)[2]åˆ©ç”¨[self-instruct](https://arxiv.org/abs/2212.10560)[3]ç”Ÿæˆçš„52K Englishi nstruction-finetuningæ•°æ®å¯¹LLaMAè¿›è¡Œäº†å¾®è°ƒã€‚ç„¶è€Œï¼Œç›®å‰è¯¥æ–¹å‘çš„ç ”ç©¶ä»ç„¶é¢ä¸´ç€ä»¥ä¸‹ä¸‰ä¸ªæŒ‘æˆ˜ï¼š
- LLaMA-7bä¾ç„¶å¯¹è®¡ç®—èµ„æºæœ‰ç€è¾ƒé«˜çš„è¦æ±‚ï¼›
- ç”¨äºinstruction finetuningçš„å¼€æºæ•°æ®é›†è¾ƒå°‘ï¼›
- ç¼ºä¹å„instructionç±»å‹å¸¦æ¥çš„å½±å“çš„å®è¯ç ”ç©¶ï¼Œå¦‚å“åº”ä¸­æ–‡çš„èƒ½åŠ›å’ŒCoTèƒ½åŠ›ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Alpaca-CoTé¡¹ç›®ï¼Œè¯¥é¡¹ç›®ç»“åˆäº†ç›¸å…³çš„è¿‘æœŸå‰æ²¿æŠ€æœ¯ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- 1. **_ä»…éœ€è¦è¾ƒä½è®¡ç®—èµ„æºå³å¯é«˜æ•ˆå®Œæˆå¯¹LLaMAçš„å¾®è°ƒ_**ã€‚`7b`,`13b`å’Œ`30b`ç‰ˆæœ¬çš„LLaMAæ¨¡å‹å‡å¯åœ¨å•å¡80G A100ä¸Šè½»æ¾å®Œæˆè®­ç»ƒã€‚è¯¥ä¼˜åŠ¿ä¸»è¦æ¥è‡ªäº[low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf) [4], [PEFT](https://github.com/huggingface/peft)å’Œ[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)ç­‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç ä¸»è¦ä¿®æ”¹è‡ª[è¿™é‡Œ](https://github.com/tloen/alpaca-lora)ã€‚
- 2. æˆ‘ä»¬å‘å¸ƒçš„æ¨¡å‹ **_æ˜¾è‘—æå‡äº†CoT(reasoning)æ¨ç†èƒ½åŠ›_**ã€‚
- 3. æˆ‘ä»¬å‘å¸ƒçš„æ¨¡å‹ **_æ˜¾è‘—æå‡äº†å¯¹ä¸­æ–‡æŒ‡ä»¤çš„å“åº”èƒ½åŠ›_**ã€‚
- 4. ç»´æŠ¤äº†ä¸€ä¸ªä»åœ¨ä¸æ–­æ‰©å¤§è§„æ¨¡çš„ **_[intruction-finetuningçš„æ•°æ®é›†é›†åˆ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)_**ã€‚è¯¥é›†åˆåŒ…å«äº†ä¸­æ–‡ã€è‹±æ–‡å’ŒCoTçš„instructionæ•°æ®ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿç»´æŠ¤äº†ä¸€ä¸ªè®­ç»ƒè‡ªå„ç§instructionæ•°æ®é›†çš„æ¨¡å‹checkpointé›†åˆã€‚
- 5. é›†æˆäº† **_å¤šç§LLMså¹¶ç»Ÿä¸€äº†è°ƒç”¨æ¥å£_**ï¼Œå¯é€šè¿‡è¶…å‚è½»æ¾åˆ‡æ¢ã€‚ç›®å‰åŒ…å« **LLaMA, ChatGLM**[5]å’Œ **Bloom**[6]ï¼Œåç»­å°†æŒç»­åŠ å…¥æ›´å¤š,ä»¥ä¾›ç ”ç©¶è€…ä»¬è½»æ¾è°ƒç”¨å’Œå¯¹æ¯”ä¸åŒLLMsã€‚
- 6. æä¾›äº† **_è¯¦å°½é€å½»çš„å®è¯å­¦ä¹ å’Œå®šæ€§åˆ†æ_**ï¼Œè¿™é‡Œçš„findingså¯èƒ½ä¼šå¯¹ä¿ƒè¿›æœªæ¥LLMæ¢ç´¢æœ‰ä¸€å®šçš„å‚è€ƒä»·å€¼ã€‚ -->



## æ•°æ®é›†åˆ (Data Collection)


<!-- æ”¶é›†æ•°æ®é›†çš„ç›¸å¯¹å¤§å°å¦‚ä¸‹å›¾æ‰€ç¤º: -->

<!-- ![img](./figures/show.png) -->


<!-- æˆ‘ä»¬å‚è€ƒ[è¿™é‡Œ](https://github.com/yaodongC/awesome-instruction-dataset) ([@yaodongC](https://github.com/yaodongC)), å°†æ”¶é›†åˆ°çš„æ•°æ®é›†æŒ‰ç…§ä»¥ä¸‹è§„åˆ™æ ‡æ³¨Tagsï¼š -->

è¯­è¨€:
- EN: English (è‹±æ–‡)
- CN: Chinese (ä¸­æ–‡)
- ML: Multiple languages (å¤šè¯­è¨€)

ä»»åŠ¡:
- NER: Named Entity Recognition (å‘½åå®ä½“è¯†åˆ«)
- RE: Relation Extraction (å…³ç³»æŠ½å–)
- EE: Event Extraction (äº‹ä»¶æŠ½å–)

<!-- (Gen)Generation-method:
- HG: [Human Generated Dataset] Datasets created by humans
- SI: [Self-Instruct] Datasets generated using self-instruct methods
- MIX: [Mixed Dataset] Dataset contains both human and machine generated data
- COL: [Collection of Dataset] Dataset made from a collection of other datasets -->

<!-- ### æ•°æ®ç»Ÿè®¡ -->
| æ•°æ®é›†     | é¢†åŸŸ       | æ•°ç›®      | è¯­è¨€      | ä»»åŠ¡       | æ¥æº                                             |
|:---------:|:---------:|:---------:|:--------:|:---------:|:------------------------------------------------:|
| DuIE2.0   | äººæ–‡       | 210K          | CN       | RE        |https://www.luge.ai/#/luge/dataDetail?id=5       |
| DuEE1.0   | æ–°é—»       | 17K          | CN       | EE        |https://www.luge.ai/#/luge/dataDetail?id=6       |
| DuEE-fin   | é‡‘è       | 11.7K          | CN       | EE        |https://www.luge.ai/#/luge/dataDetail?id=7       |
| IREE   | é‡‘è       | 50K          | CN       | EE        |https://www.luge.ai/#/luge/dataDetail?id=72       |
| SanWen   | ä¸­å›½æ–‡å­¦       | 21K          | CN       | RE        |https://github.com/thunlp/Chinese_NRE/tree/master/data/SanWen       |
| BosonNER   | é€šç”¨       | 120K          | CN       | NER        |https://github.com/HuHsinpang/BosonNER-Pretreatment/tree/master/boson/data   |
| MSRANER   | é€šç”¨       | 50K          | CN       | NER        |https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA       |
| FinRe   | é‡‘è       | 18K          | CN       | RE        |https://github.com/thunlp/Chinese_NRE/tree/master/data/FinRE      |








<!-- | å•å…ƒæ ¼4 | å•å…ƒæ ¼5 | å•å…ƒæ ¼6 | -->


<!-- è¯¥é›†åˆä»åœ¨ä¸æ–­æ›´æ–°å’Œæ‰©å¢ä¸­ã€‚å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸‹è½½å’ŒæŸ¥çœ‹æ›´å¤šæ•°æ®ç»†èŠ‚ï¼šhttps://github.com/PhoebusSi/alpaca-CoT/tree/main/data -->



<!-- ### ä¸‹è½½
ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main)ä¸‹è½½æ‰€æœ‰æˆ‘ä»¬å·²ç»ç»Ÿä¸€æ ¼å¼åçš„formattedæ•°æ®ã€‚ç„¶åï¼Œå°†ä¸‹è½½åˆ°çš„æ–‡ä»¶å…¨éƒ¨æ”¾åˆ°[data](https://github.com/PhoebusSi/alpaca-CoT/tree/main/data) folderã€‚

ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main)ä¸‹è½½è®­ç»ƒè‡ªå„ç§ç±»å‹instructionæ•°æ®çš„æ‰€æœ‰checkpontsã€‚ç„¶åï¼Œåœ¨`generate.py`ä¸­çš„`LoRA_Weights`è®¾ç½®æˆä¸‹è½½è·¯å¾„ï¼Œå³å¯ç›´æ¥è¿è¡Œæ¨¡å‹çš„inferenceä»¥æŸ¥çœ‹æ¨¡å‹æ•ˆæœã€‚ -->

### æ•°æ®æ ¼å¼
<!-- æˆ‘ä»¬é›†åˆä¸­çš„æ‰€æœ‰æ•°æ®å‡å·²è¢«è½¬åŒ–æˆç›¸åŒçš„æ ¼å¼ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ ¼å¼å¦‚ä¸‹ï¼š
```
[
{"instruction": instruction string,
"input": input string, # (may be empty)
"output": output string}
]
```
æ³¨æ„ï¼Œå¯¹äºCoTæ•°æ®é›†,æˆ‘ä»¬é¦–å…ˆä½¿ç”¨FLANæä¾›çš„[template](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py)å°†å…¶ä»åŸæ•°æ®è½¬åŒ–æˆChain-of-Thoughtçš„å½¢å¼ï¼Œä¹‹åå†ç»Ÿä¸€æˆä»¥ä¸Šæ ¼å¼ã€‚æ ¼å¼ç»Ÿä¸€åŒ–çš„è„šæœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/PhoebusSi/alpaca-CoT/blob/main/data/origin_cot_data/formating.py)æ‰¾åˆ°ã€‚  -->


<!-- ## 4. å¤šæ¥å£ç»Ÿä¸€çš„å¼€æºå¹³å°

### ç¯å¢ƒé…ç½®
```
pip install -r requirements.txt
```
### æ¨¡å‹å¾®è°ƒ
ä¸ºäº†ä¾¿äºç ”ç©¶è€…ä»¬åœ¨LLMä¸Šåšç³»ç»Ÿçš„IFTç ”ç©¶ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸åŒç±»å‹çš„instructionæ•°æ®ï¼Œé›†æˆäº†å¤šç§LLMï¼Œå¹¶ç»Ÿä¸€äº†æ¥å£ï¼Œå¯ä»¥è½»æ¾å®šåˆ¶åŒ–æƒ³è¦çš„æ­é…ï¼š
- `--model_type`: è®¾ç½®æƒ³è¦ç ”ç©¶çš„LLMï¼Œç›®å‰å·²æ”¯æŒ[llama, chatglmå’Œbloom]ï¼Œå…¶ä¸­åä¸¤è€…çš„ä¸­æ–‡èƒ½åŠ›è¾ƒå¼ºï¼Œåç»­å°†ä¼šé›†æˆæ›´å¤šçš„LLMsã€‚
- `--data`: è®¾ç½®ç”¨ä»¥IFTçš„æ•°æ®ç±»å‹ï¼Œä»¥çµæ´»ç‰¹åˆ¶æƒ³è¦çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¦‚è¿½æ±‚è¾ƒå¼ºçš„æ¨ç†èƒ½åŠ›å¯è®¾ç½®alpaca-cotï¼Œè¾ƒå¼ºçš„ä¸­æ–‡èƒ½åŠ›å¯è®¾ç½®belle1.5mï¼Œè¾ƒå¼ºçš„codingå’Œæ•…äº‹åˆ›ä½œèƒ½åŠ›å¯è®¾ç½®gpt4allï¼Œé‡‘èç›¸å…³çš„å“åº”èƒ½åŠ›å¯è®¾ç½®financeã€‚
- `--model_name_or_path`: ä¸`--model_type`ç›¸å¯¹åº”ï¼Œç”¨æ¥åŠ è½½ç›®æ ‡LLMçš„ä¸åŒå‹å·æƒé‡ã€‚å¦‚ï¼Œè¦åŠ è½½llamaçš„13bçš„æ¨¡å‹æƒé‡æ—¶å¯è®¾ç½®decapoda-research/llama-13b-hfã€‚ 

**å•å¡**
- for LLaMA
```
python3 uniform_finetune.py --model_type llama --model_name_or_path decapoda-research/llama-7b-hf \
    --data alpaca-belle-cot --lora_target_modules q_proj v_proj \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1 
    
```

- for ChatGLM
Note: for multiple datasets, you can use `--data` like `--data ./data/alpaca.json ./data/finance.json <path2yourdata_1>`
```
python3 uniform_finetune.py   --model_type chatglm --model_name_or_path THUDM/chatglm-6b \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --lora_r 32 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 2 \
    --learning_rate 2e-5 --epochs 1
```
Note that `load_in_8bit` is not yet suitable for ChatGLM, so batch_size must be much smaller than others. 

- for BLOOM
```
python3 uniform_finetune.py   --model_type bloom --model_name_or_path bigscience/bloomz-7b1-mt \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1 
```

Note that you can also pass the local path (where the LLM weights saved) to `--model_name_or_path`. And the data type `--data` can be freely set according to your interests.

**å¤šå¡**
- for LLaMA
```
python3 -m torch.distributed.launch --nproc_per_node 4  \
    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy uniform_finetune.py \
    --model_type llama --model_name_or_path decapoda-research/llama-7b-hf \
    --data alpaca-belle-cot --lora_target_modules q_proj v_proj \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1 
```
- for ChatGLM
```
python3 -m torch.distributed.launch --nproc_per_node 4  \
    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy \
    uniform_finetune.py   --model_type chatglm --model_name_or_path THUDM/chatglm-6b \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --lora_r 32 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 2 \
    --learning_rate 2e-5 --epochs 1
```
Note that `load_in_8bit` is not yet suitable for ChatGLM, so batch_size must be much smaller than others. 

- for BLOOM
```
python3 -m torch.distributed.launch --nproc_per_node 4  \
    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy \
    uniform_finetune.py   --model_type bloom --model_name_or_path bigscience/bloomz-7b1-mt \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1  
```
### Inference
``` 
python3 generate.py  --data alpaca-belle-cot --model_type llama

python3 generate.py  --data alpaca-belle-cot --model_type chatglm

python3 generate.py  --data alpaca-belle-cot --model_type bloom
```

æ³¨æ„ï¼Œ`saved-xxx7b` æ–‡ä»¶å¤¹æ˜¯ä¿å­˜LoRA weightsçš„è·¯å¾„ï¼Œè€ŒLLaMAçš„weightsåˆ™ä¼šåœ¨è„šæœ¬æ‰§è¡ŒæœŸé—´è‡ªåŠ¨ä»Hugging Faceä¸Šä¸‹è½½ã€‚

### ç”Ÿæˆç›¸å…³è¶…å‚è®¾ç½®
```
top_p=0.9, 
        #é€‚åº¦è°ƒå¤§æ ¸é‡‡æ ·çš„æ¦‚ç‡é˜ˆå€¼ï¼Œæ‰©å¤§å€™é€‰å­é›†ï¼Œå¢åŠ ç”Ÿæˆå¤šæ ·æ€§ã€‚
        
temperature=1.0, 
        #ä¹‹å‰çš„æ¸©åº¦å‚æ•°è¿‡ä½ä¼šå¯¼è‡´ç”Ÿæˆè¯çš„æ¦‚ç‡åˆ†å¸ƒæåŒ–ä¸¥é‡ï¼Œå¯¼è‡´ç”Ÿæˆç­–ç•¥é€€åŒ–æˆgreedy decodingã€‚
        
do_sample=True, 
        #do_sampleå‚æ•°é»˜è®¤å…³é—­ï¼Œä¸å¼€å¯æ—¶ç”Ÿæˆä»ä¿æŒbeam-searchè§£ç ç­–ç•¥ï¼Œå¼€å¯åä¸ºbeam-search multinomial samplingè§£ç ç­–ç•¥ã€‚
        
no_repeat_ngram_size=6, 
        #é€šè¿‡é…ç½®ä¸‹ä¸€ä¸ªè¯é‡å¤å‡ºç°n-gramçš„æ¦‚ç‡ä¸º0ï¼Œæ¥ä¿è¯æ²¡æœ‰n-gramå‡ºç°ä¸¤æ¬¡ï¼Œè®¾ç½®è¿‡å°ä¼šæŠ‘åˆ¶åˆç†çš„é‡å¤ï¼Œå½±å“ç”Ÿæˆçš„æµç•…æ€§ï¼Œè¿‡å¤§ä¼šå¤±å»ä½œç”¨ã€‚
        
repetition_penalty=1.8, 
        #å¯¹äºä¹‹å‰å‡ºç°è¿‡çš„è¯è¯­ï¼Œåœ¨åç»­é¢„æµ‹çš„è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å¼•å…¥æƒ©ç½šå› å­repetition_penaltyé™ä½å…¶å‡ºç°çš„æ¦‚ç‡ã€‚
```

### å‚æ•°åˆå¹¶
```
python3 merge.py --model_type llama --size 7b --lora_dir xxx --merged_dir yyy
```
### æœ¬åœ°ä½¿ç”¨
```
python3 server.py --model_type chatglm --lora_dir xxx
```
### æ‰¹é‡é¢„æµ‹
```
python3 predict.py --model_type chatglm --data for_dict_data --lora_dir xxx --result_dir yyy
```

### webæœåŠ¡

```
python3 web.py --model_type chatglm --lora_dir xxx
```


## 5. Quantitative Analysis
æ³¨æ„ï¼šä¸‹å›¾æ˜¯æˆªæ­¢åˆ°3.26æ—¥æ”¶é›†åˆ°çš„æ•°æ®é›†çš„ç»Ÿè®¡æƒ…å†µï¼Œä»…ä½œä¸ºmotivationå±•ç¤ºã€‚ç›®å‰å·²æ”¶é›†äº†æ›´å¤šæ•°æ®é›†ï¼Œå¦‚é‡‘èç›¸å…³çš„æŒ‡ä»¤æ•°æ®é›†ã€‚
![data collection statistics](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/piechart.png)
å½“å‰çš„instruction-finetuningæ•°æ®é›†åˆä¸»è¦åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š
- `alpaca_data_cleaned.json`: about 52K English instruction-following training samples.
- `CoT_data.json`: 9 CoT datasets involving about 75k samples. ï¼ˆç›¸å…³çš„CoTæ•°æ®é›†ç”±FLAN[7]å‘å¸ƒï¼‰
- `belle_data_cn.json`:  about 0.5M Chinese |instruction-following training samples. ï¼ˆç›¸å…³çš„ä¸­æ–‡instructionæ•°æ®ç”±BELLE[8]å‘å¸ƒï¼‰

### å…³äºCoTå’ŒChinese Instructionsçš„æ¶ˆè
"w/o CoT" and "w/o CN" åˆ†åˆ«è¡¨ç¤ºç”¨åœ¨instruction-finetuningæœŸé—´ä¸é‡‡ç”¨CoTæ•°æ®å’ŒChinese instructionsã€‚

éœ€è¦æ¨ç†èƒ½åŠ›çš„é—®é¢˜ä¸Šçš„è¡¨ç°
![f3](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾3.png)
 
éœ€è¦éµå¾ªä¸­æ–‡æŒ‡ä»¤çš„é—®é¢˜ä¸Šçš„è¡¨ç°
![f4](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾4.png)
 
åœ¨è¾ƒå¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°
![f5](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾5.png)

          

**In summary, the models finetuned from our complete dataset (English, Chinese, and CoT instruction data) can significantly improve model reasoning and Chinese instruction following abilities.**


### æ›´å¤šèƒ½åŠ›å±•ç¤º

![ablation-cot](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾6.png)


![ablation-cot](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾8.png) -->





## å‚è€ƒæ–‡çŒ®

<!-- [1]: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)

[2]: [Stanford Alpaca: An Instruction-following LLaMA model](https://github.com/tatsu-lab/stanford_alpaca)

[3]: [Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/abs/2212.10560)

[4]: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)

[5]: [ChatGLM: An Open Bilingual Dialogue Language Model](https://github.com/THUDM/ChatGLM-6B)

[6]: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)

[7]: [FLAN: Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

[8]: [BELLE: Bloom-Enhanced Large Language model Engine](https://github.com/LianjiaTech/BELLE)

[9]: [GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo](https://github.com/nomic-ai/gpt4all) -->



<!-- ## Citation
Please cite the repo if you use the data collection, code, and experimental findings in this repo. 
```
@misc{alpaca-cot,
  author = {Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, Zheng Lin },
  school = {Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China},
  title = {Alpaca-CoT: An Instruction Fine-Tuning Platform with Instruction Data Collection and Unified Large Lnguage Models Interface},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/PhoebusSi/alpaca-CoT}},
}
``` -->

## To do

### æ•°æ®æ”¶é›†é˜¶æ®µ
1. å°½å¯èƒ½æ”¶é›†å¹¶æ•´ç†ç°æœ‰çš„ä¿¡æ¯æŠ½å–ç›¸å…³çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸­æ–‡åŠè‹±æ–‡ã€‚
2. å°†è‹±æ–‡æ•°æ®é›†é€šè¿‡æœºå™¨ç¿»è¯‘æ¨¡å‹ç¿»è¯‘æˆä¸­æ–‡ã€‚
3. æ„å»ºæ¨¡å‹ä»¥è¿›è¡Œæ•°æ®çš„è‡ªåŠ¨åŒ–æ¸…æ´—å’Œè´¨é‡æ§åˆ¶ã€‚

### æ•°æ®æ„å»ºé˜¶æ®µ
1. é’ˆå¯¹ä¸åŒçš„ä¿¡æ¯æŠ½å–ä»»åŠ¡ï¼Œæ„å»ºä¸åŒçš„instructionsã€‚
2. å°†æ•°æ®æ ¼å¼ç»Ÿä¸€ï¼Œå¹¶åŠ å…¥instructionsï¼Œç”Ÿæˆå¤§å‹ä¸­æ–‡ä¿¡æ¯æŠ½å–æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚


<!-- Chinchillaçš„ç‰¹ç‚¹ï¼š

ç»†è‡´å…¥å¾®ï¼š Chinchillaï¼ˆæ —é¼ ï¼‰çš„æ¯›çš®éå¸¸ç»†è…»ï¼Œæ¯ä¸€ä¸ªæ¯›å›Šå¯ä»¥é•¿å‡º40-80æ ¹æ¯›å‘ã€‚è¿™å¯ä»¥è±¡å¾ä½ çš„æ¨¡å‹åœ¨è¿›è¡Œä¿¡æ¯æŠ½å–æ—¶å¯¹ç»†èŠ‚çš„æŠŠæ¡ï¼Œä»¥åŠå¯¹å¤§é‡æ•°æ®å¤„ç†çš„èƒ½åŠ›ã€‚

çµæ´»é€‚åº”ï¼š æ —é¼ åœ¨é‡å¤–ç¯å¢ƒä¸­éœ€è¦é€‚åº”å¤šç§å¤æ‚çš„ç¯å¢ƒï¼Œè¿™ä¹Ÿåæ˜ äº†ä½ çš„æ¨¡å‹åœ¨å¤„ç†å„ç§ä¸åŒç±»å‹å’Œç»“æ„çš„æ•°æ®æ—¶çš„çµæ´»æ€§ã€‚

çè´µä¸ç¨€æœ‰ï¼š ç”±äºè¿‡åº¦æ•çŒï¼Œæ —é¼ ç°åœ¨åœ¨é‡å¤–å·²ç»å¾ˆç¨€æœ‰ï¼Œè¢«è§†ä¸ºçè´µçš„åŠ¨ç‰©ã€‚è¿™å¯ä»¥è±¡å¾ä½ çš„æ¨¡å‹çš„ç‹¬ç‰¹æ€§å’Œå®ƒåœ¨ä¿¡æ¯æŠ½å–é¢†åŸŸçš„ä»·å€¼ã€‚

ç”Ÿæ´»ä¹ æ€§ï¼š æ —é¼ æ˜¯ç¾¤å±…åŠ¨ç‰©ï¼Œå®ƒä»¬åœ¨ä¸€èµ·ç”Ÿæ´»ï¼Œåˆ†äº«èµ„æºã€‚æˆ‘ä»¬ä¹Ÿå¸Œæœ›å¤§å®¶ä¸€èµ·æˆé•¿ï¼Œå…±åŒè¿›æ­¥ï¼Œåˆ†äº«èµ„æºã€‚

å‡ºè‰²çš„å­¦ä¹ èƒ½åŠ›ï¼š æ —é¼ çš„å­¦ä¹ èƒ½åŠ›éå¸¸å‡ºè‰²ï¼Œå¯ä»¥å¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒï¼Œè¿™ä¹Ÿè±¡å¾ä½ çš„æ¨¡å‹å…·æœ‰é«˜æ•ˆçš„å­¦ä¹ å’Œé€‚åº”èƒ½åŠ›ã€‚ -->